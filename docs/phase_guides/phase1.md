# Phase 1 Guide: Foundation & Basic RAG Setup

## ğŸ“‹ Overview

Phase 1 establishes the foundation of the Multi-Source Research Intelligence Platform (MSRIP). This phase implements a complete RAG (Retrieval-Augmented Generation) pipeline for ArXiv academic papers, including data fetching, document processing, chunking, embedding, and vector storage.

**Status**: âœ… Complete  
**Version**: 1.0.0-phase1  
**Date**: December 2024

---

## ğŸ¯ Phase 1 Objectives

### Primary Goals
1. âœ… **ArXiv Integration**: Fetch and store ArXiv paper metadata and PDFs
2. âœ… **Document Processing**: Load, chunk, and embed documents
3. âœ… **Vector Store**: Create persistent vector database with ChromaDB
4. âœ… **Basic RAG**: Enable question-answering with citation tracking
5. âœ… **Data Persistence**: Save all processed data for reproducibility

### Success Criteria
- âœ… Fetch 100+ ArXiv papers successfully
- âœ… Process documents with proper metadata preservation
- âœ… Generate embeddings for all document chunks
- âœ… Create persistent vector store
- âœ… All 55+ unit and integration tests passing
- âœ… Complete documentation and guides

---

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ArXiv Fetcher  â”‚ â†’ Fetches metadata and PDFs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document Loader â”‚ â†’ Combines metadata + PDF text
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document Chunkerâ”‚ â†’ Splits into manageable chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embedder      â”‚ â†’ Generates vector embeddings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Store   â”‚ â†’ ChromaDB for similarity search
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Fetch**: `scripts/fetch_arxiv_data.py` â†’ `data/raw/arxiv_metadata/`
2. **Load**: Metadata + PDFs â†’ `Document` objects
3. **Chunk**: Documents â†’ Smaller chunks with metadata
4. **Embed**: Chunks â†’ Vector embeddings (384-dim)
5. **Store**: Embeddings â†’ ChromaDB vector store

---

## ğŸš€ Getting Started

### Prerequisites

```bash
# Python 3.10+
python --version

# Virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 1: Fetch ArXiv Data

**âš ï¸ CRITICAL**: You must fetch data before running the embedding pipeline. No data = No chunks = No embeddings!

```bash
# Fetch 100 papers (metadata only)
python scripts/fetch_arxiv_data.py --max-results 100

# Fetch 500 papers with PDFs (for full-text processing)
python scripts/fetch_arxiv_data.py --max-results 500 --download-pdfs

# Fetch in batches (recommended for large datasets)
python scripts/fetch_arxiv_data.py --batches 5 --results-per-batch 100 --delay 5.0
```

**What this does**:
- Fetches metadata from ArXiv API
- Saves metadata to `data/raw/arxiv_metadata/<arxiv_id>.json`
- Optionally downloads PDFs to `data/raw/arxiv_pdfs/`
- Respects rate limits (5 second delay between batches)

**Expected Output**:
```
[INFO] [arxiv_fetcher] Starting batched ArXiv fetch
[INFO] [arxiv_fetcher] Fetched ArXiv batch
[INFO] [arxiv_fetcher] Saving ArXiv metadatas to disk
[INFO] [arxiv_fetcher] Saved ArXiv metadata
```

### Step 2: Run Embedding Pipeline

#### Option A: Using Jupyter Notebook (Recommended for Exploration)

```bash
# Start Jupyter
jupyter notebook notebooks/02_embedding_pipeline.ipynb
```

The notebook includes:
- âœ… Data fetching instructions
- âœ… Step-by-step processing
- âœ… Error handling and validation
- âœ… Results visualization

#### Option B: Using Python Script

```python
from src.embedding.document_processor import process_arxiv_abstracts

# Process abstracts only (faster, no PDFs needed)
documents = process_arxiv_abstracts(max_documents=100)

# Or process full papers (requires PDFs)
from src.embedding.document_processor import DocumentProcessor

processor = DocumentProcessor()
documents = processor.process_documents(
    include_full_text=True,
    max_documents=100,
    save_to_disk=True
)
```

### Step 3: Verify Results

```bash
# Check processed data
ls -lh data/processed/arxiv/

# Check vector store
ls -lh vector_db/

# Run tests
pytest tests/ -v
```

---

## ğŸ“ Data Structure

### Raw Data (`data/raw/`)

```
data/raw/
â”œâ”€â”€ arxiv_metadata/
â”‚   â”œâ”€â”€ 1808.01591v1.json    # Individual metadata files
â”‚   â”œâ”€â”€ 2001.00001v1.json
â”‚   â””â”€â”€ ...
â””â”€â”€ arxiv_pdfs/              # Optional: PDF files
    â”œâ”€â”€ 1808.01591v1.pdf
    â””â”€â”€ ...
```

**Metadata File Format**:
```json
{
  "id": "1808.01591v1",
  "title": "LISA: Explaining Recurrent Neural Network Judgments",
  "summary": "Recurrent neural networks...",
  "authors": ["Pankaj Gupta", "Hinrich SchÃ¼tze"],
  "published": "2018-08-05T00:00:00Z",
  "updated": "2018-08-05T00:00:00Z",
  "links": ["https://arxiv.org/abs/1808.01591v1"],
  "categories": ["cs.CL", "cs.AI"],
  "pdf_url": "https://arxiv.org/pdf/1808.01591v1.pdf",
  "fetched_at": "2024-01-15T10:00:00Z",
  "source": "arxiv"
}
```

### Processed Data (`data/processed/`)

```
data/processed/
â”œâ”€â”€ arxiv/
â”‚   â”œâ”€â”€ documents/           # Individual loaded documents
â”‚   â”‚   â”œâ”€â”€ 1808.01591v1_2024-01-15_10-00-00_abstract_only.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ chunks/              # Processed chunks with embeddings
â”‚       â”œâ”€â”€ arxiv_chunks_2024-01-15_10-00-00_abstracts.json
â”‚       â””â”€â”€ arxiv_chunks_2024-01-15_10-00-00_full_text.json
```

**Chunk File Format**:
```json
{
  "source": "arxiv",
  "chunks": [
    {
      "chunk_id": "1808.01591v1_chunk_0",
      "chunk_index": 0,
      "page_content": "Title: LISA...",
      "metadata": {
        "arxiv_id": "1808.01591v1",
        "title": "LISA: Explaining...",
        "authors": ["Pankaj Gupta", "Hinrich SchÃ¼tze"],
        "published": "2018-08-05T00:00:00Z",
        "pdf_url": "https://arxiv.org/pdf/1808.01591v1.pdf",
        "source": "arxiv",
        "embedding": [0.123, -0.456, ...],  # 384-dim vector
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2"
      }
    }
  ]
}
```

---

## ğŸ”§ Configuration

### Environment Variables (`.env`)

```bash
# Optional: OpenAI API key (for future LLM integration)
OPENAI_API_KEY=sk-...

# Optional: Anthropic API key
ANTHROPIC_API_KEY=sk-ant-...
```

### Config File (`src/config.py`)

Key settings:
- `RAW_DATA_DIR`: `data/raw/`
- `PROCESSED_DATA_DIR`: `data/processed/`
- `VECTOR_DB_DIR`: `vector_db/`
- `CHUNK_SIZE`: 1000 (characters)
- `CHUNK_OVERLAP`: 200 (characters)

---

## ğŸ“Š Key Components

### 1. ArXiv Fetcher (`src/ingestion/arxiv_fetcher.py`)

**Functions**:
- `ArxivFetcher(max_results=100)`: Fetch single batch
- `batch_fetch_arxiv_metadata(...)`: Fetch multiple batches
- `save_metadatas(metadatas)`: Save to disk
- `download_pdfs_from_metadatas_file()`: Download PDFs

**Usage**:
```python
from src.ingestion.arxiv_fetcher import (
    batch_fetch_arxiv_metadata,
    save_metadatas
)

# Fetch metadata
metadata = batch_fetch_arxiv_metadata(
    num_batches=5,
    results_per_batch=100,
    delay_seconds=5.0
)

# Save to disk
save_metadatas(metadata)
```

### 2. Document Loader (`src/ingestion/document_loader.py`)

**Class**: `DocumentLoader`

**Methods**:
- `load_metadata(arxiv_id)`: Load single metadata file
- `load_all_documents(include_full_text=False, max_documents=None)`: Load all documents
- `_save_loaded_documents(documents, include_full_text)`: Save loaded documents

**Usage**:
```python
from src.ingestion.document_loader import DocumentLoader

loader = DocumentLoader()

# Load abstracts only (no PDFs needed)
documents = loader.load_all_documents(
    include_full_text=False,
    max_documents=100
)

# Load full papers (requires PDFs)
documents = loader.load_all_documents(
    include_full_text=True,
    max_documents=100,
    save_to_disk=True  # Save individual documents
)
```

### 3. Document Chunker (`src/embedding/chunking.py`)

**Class**: `DocumentChunker`

**Strategies**:
- `ChunkingStrategy.NONE`: No chunking (use entire document)
- `ChunkingStrategy.RECURSIVE`: Recursive character splitting
- `ChunkingStrategy.TOKEN`: Token-based splitting

**Usage**:
```python
from src.embedding.chunking import DocumentChunker, ChunkingStrategy

chunker = DocumentChunker(
    strategy=ChunkingStrategy.RECURSIVE,
    chunk_size=1000,
    chunk_overlap=200
)

chunks = chunker.chunk_documents(documents)
```

### 4. Document Embedder (`src/embedding/embedder.py`)

**Class**: `DocumentEmbedder`

**Models**:
- Default: `all-MiniLM-L6-v2` (384 dimensions)
- Fast, CPU-friendly embeddings

**Usage**:
```python
from src.embedding.embedder import DocumentEmbedder

embedder = DocumentEmbedder(model_name="all-MiniLM-L6-v2")
embedded_docs = embedder.embed_documents(documents, batch_size=32)
```

### 5. Document Processor (`src/embedding/document_processor.py`)

**Class**: `DocumentProcessor` - Orchestrates the full pipeline

**Convenience Function**: `process_arxiv_abstracts(max_documents=None)`

**Usage**:
```python
from src.embedding.document_processor import (
    DocumentProcessor,
    process_arxiv_abstracts
)

# Quick way: Process abstracts
docs = process_arxiv_abstracts(max_documents=100)

# Full control: Custom processing
processor = DocumentProcessor(
    embedding_model="all-MiniLM-L6-v2",
    chunk_strategy=ChunkingStrategy.NONE
)

docs = processor.process_documents(
    include_full_text=False,
    max_documents=100,
    batch_size=32,
    save_to_disk=True
)
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
# Activate virtual environment
source .venv/bin/activate

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

### Test Structure

```
tests/
â”œâ”€â”€ conftest.py                    # Shared fixtures
â”œâ”€â”€ test_arxiv_fetcher.py          # ArXiv fetching tests
â”œâ”€â”€ test_document_loader.py        # Document loading tests
â”œâ”€â”€ test_chunking.py               # Chunking tests
â”œâ”€â”€ test_embedder.py               # Embedding tests
â”œâ”€â”€ test_document_processor.py    # Pipeline tests
â”œâ”€â”€ test_chunk_saver.py            # Data persistence tests
â”œâ”€â”€ test_integration.py            # End-to-end tests
â””â”€â”€ test_setup.py                  # Project setup tests
```

### Test Coverage

- âœ… **Unit Tests**: 45+ tests covering individual components
- âœ… **Integration Tests**: 10+ tests for end-to-end workflows
- âœ… **Coverage**: >70% code coverage

---

## ğŸ› Troubleshooting

### Problem: "No documents were loaded"

**Cause**: No metadata files in `data/raw/arxiv_metadata/`

**Solution**:
```bash
# Fetch metadata first
python scripts/fetch_arxiv_data.py --max-results 100
```

### Problem: "Metadata not found for <arxiv_id>"

**Cause**: Metadata file doesn't exist or wrong path

**Solution**:
```bash
# Check if metadata exists
ls data/raw/arxiv_metadata/<arxiv_id>.json

# Re-fetch if missing
python scripts/fetch_arxiv_data.py --max-results 100
```

### Problem: "No PDFs found" (when using full_text=True)

**Cause**: PDFs not downloaded

**Solution**:
```bash
# Download PDFs
python scripts/fetch_arxiv_data.py --max-results 100 --download-pdfs
```

### Problem: Rate limiting errors

**Cause**: Too many API calls too quickly

**Solution**:
```bash
# Use batch fetching with delays
python scripts/fetch_arxiv_data.py --batches 5 --results-per-batch 100 --delay 5.0
```

### Problem: Embedding model download fails

**Cause**: Network issues or disk space

**Solution**:
```bash
# Check disk space
df -h

# Try again (model is cached after first download)
python -c "from src.embedding.embedder import DocumentEmbedder; DocumentEmbedder()"
```

---

## ğŸ“ˆ Performance Benchmarks

### Phase 1 Metrics

| Operation | Time | Notes |
|-----------|------|-------|
| Fetch 100 papers | ~30s | With 5s delay between batches |
| Load 100 abstracts | ~1s | No PDF processing |
| Load 100 full papers | ~5-10min | PDF extraction is slow |
| Chunk 100 documents | ~2s | Recursive strategy |
| Embed 100 chunks | ~10s | CPU, batch_size=32 |
| Full pipeline (abstracts) | ~15s | End-to-end |
| Full pipeline (full text) | ~6-12min | End-to-end with PDFs |

### Resource Usage

- **Memory**: ~500MB-1GB (depending on batch size)
- **Disk**: ~50MB per 100 papers (metadata only), ~500MB with PDFs
- **CPU**: Single-threaded (CPU-friendly embeddings)

---

## âœ… Phase 1 Completion Checklist

- [x] ArXiv fetcher with rate limiting
- [x] Document loader (abstracts + full text)
- [x] Document chunker (multiple strategies)
- [x] Document embedder (HuggingFace models)
- [x] Document processor (orchestration)
- [x] Data persistence (save/load chunks)
- [x] Vector store integration (ChromaDB)
- [x] Comprehensive test suite (55+ tests)
- [x] Documentation and guides
- [x] Error handling and logging
- [x] Constants and enums for maintainability

---

## ğŸ”„ Next Steps: Phase 2

Phase 2 will add:
- Multi-source integration (Semantic Scholar, PubMed)
- Router agent for source selection
- Enhanced metadata handling
- Cross-source citation tracking

See `docs/phase_guides/phase2.md` (coming soon) for details.

---

## ğŸ“š Additional Resources

- [AGENT.md](../AGENT.md) - Architecture rules and guidelines
- [README.md](../../README.md) - Project overview
- [API Reference](api_reference.md) - Detailed API documentation
- [Scripts README](../../scripts/README.md) - Script usage guide

---

## ğŸ¤ Support

For issues or questions:
1. Check this guide first
2. Review error logs in `logs/`
3. Run tests to verify setup
4. Check GitHub issues

---

**Last Updated**: December 2024  
**Phase Status**: âœ… Complete

