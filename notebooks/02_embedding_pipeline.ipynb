{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995500a1",
   "metadata": {},
   "source": [
    "# Phase 1: Embedding Pipeline\n",
    "\n",
    "This notebook demonstrates the complete document processing pipeline for Phase 1:\n",
    "1. **Data Fetching** (ArXiv metadata and PDFs)\n",
    "2. **Document Loading** (combining metadata with text)\n",
    "3. **Document Chunking** (splitting into manageable pieces)\n",
    "4. **Document Embedding** (generating vector representations)\n",
    "5. **Data Persistence** (saving processed chunks)\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANT: Fetch Data First!\n",
    "\n",
    "**Before running this notebook, you MUST fetch ArXiv data first!**\n",
    "\n",
    "No data = No chunks = No embeddings\n",
    "\n",
    "Run this command in your terminal:\n",
    "```bash\n",
    "# Fetch 100 papers (metadata only - for abstracts)\n",
    "python scripts/fetch_arxiv_data.py --max-results 100\n",
    "\n",
    "# OR fetch with PDFs (for full-text processing)\n",
    "python scripts/fetch_arxiv_data.py --max-results 100 --download-pdfs\n",
    "```\n",
    "\n",
    "See `docs/phase_guides/phase1.md` for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d6312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [embedding_pipeline] Embedding pipeline notebook initialized\n"
     ]
    }
   ],
   "source": [
    "# Setup: Add src to path and configure logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Imports\n",
    "import src.config\n",
    "from src.utils.logging_config import setup_logging\n",
    "from src.embedding.document_processor import DocumentProcessor, process_arxiv_abstracts\n",
    "from src.constants import ChunkingStrategy\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging('embedding_pipeline')\n",
    "logger.info('Embedding pipeline notebook initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c135d9a",
   "metadata": {},
   "source": [
    "## Step 1: Verify Data Availability\n",
    "\n",
    "Before processing, let's check if we have ArXiv metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be22df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Metadata directory: /mnt/data/lourvens/learning/research-agent/data/raw/arxiv_metadata\n",
      "üìÑ Found 100 metadata files\n",
      "‚úÖ Data ready! Sample files:\n",
      "   - 2010.09254v1.json\n",
      "   - 1808.02632v1.json\n",
      "   - 2011.02705v1.json\n",
      "   - 1808.10568v2.json\n",
      "   - 2010.00247v2.json\n",
      "   ... and 95 more\n"
     ]
    }
   ],
   "source": [
    "# Check if metadata files exist\n",
    "from src import config\n",
    "from src.constants import ARXIV_METADATA_SUBDIR\n",
    "\n",
    "metadata_dir = config.RAW_DATA_DIR / ARXIV_METADATA_SUBDIR\n",
    "metadata_files = list(metadata_dir.glob(\"*.json\")) if metadata_dir.exists() else []\n",
    "\n",
    "print(f\"üìÅ Metadata directory: {metadata_dir}\")\n",
    "print(f\"üìÑ Found {len(metadata_files)} metadata files\")\n",
    "\n",
    "if len(metadata_files) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No metadata files found!\")\n",
    "    print(\"\\nPlease run this command in your terminal first:\")\n",
    "    print(\"  python scripts/fetch_arxiv_data.py --max-results 100\")\n",
    "    print(\"\\nOr see docs/phase_guides/phase1.md for detailed instructions.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data ready! Sample files:\")\n",
    "    for f in metadata_files[:5]:\n",
    "        print(f\"   - {f.name}\")\n",
    "    if len(metadata_files) > 5:\n",
    "        print(f\"   ... and {len(metadata_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3191a65",
   "metadata": {},
   "source": [
    "## Step 2: Process Documents\n",
    "\n",
    "Choose one of the following options:\n",
    "\n",
    "### Option A: Process Abstracts Only (Fast, No PDFs Required)\n",
    "- Uses only metadata (title + abstract)\n",
    "- Fast processing (~15 seconds for 100 papers)\n",
    "- No PDF downloads needed\n",
    "\n",
    "### Option B: Process Full Papers (Slower, Requires PDFs)\n",
    "- Includes full PDF text\n",
    "- Slower processing (~5-10 minutes for 100 papers)\n",
    "- Requires PDFs to be downloaded first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff7136e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing abstracts only...\n",
      "============================================================\n",
      "[INFO] [pdf_processor] Initialized PDFProcessor with loader: pymupdf\n",
      "[INFO] [chunking] Initialized DocumentChunker\n",
      "[INFO] [embedder] Initializing DocumentEmbedder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/lourvens/learning/research-agent/src/embedding/embedder.py:40: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [embedder] DocumentEmbedder initialized successfully\n",
      "[INFO] [document_processor] Initialized DocumentProcessor pipeline\n",
      "[INFO] [document_processor] Starting document processing pipeline\n",
      "[INFO] [document_loader] Loading all documents\n",
      "[INFO] [document_loader] Found 10 metadata files\n",
      "[INFO] [document_loader] Completed document loading\n",
      "[INFO] [document_loader] Saved loaded documents (one file per document)\n",
      "[INFO] [document_loader] Saved loaded documents for testing/monitoring\n",
      "[INFO] [chunking] Chunking 10 documents\n",
      "[INFO] [chunking] Completed chunking\n",
      "[INFO] [embedder] Starting document embedding\n",
      "[INFO] [embedder] Completed document embedding\n",
      "[INFO] [chunk_saver] Saved processed chunks\n",
      "[INFO] [document_processor] Saved processed chunks to /mnt/data/lourvens/learning/research-agent/data/processed/arxiv/chunks/arxiv_chunks_2025-12-16_10-48-16_abstracts.json\n",
      "[INFO] [document_processor] Document processing pipeline completed\n",
      "\n",
      "‚úÖ Processed 10 documents\n",
      "üìä Embedding dimension: 384\n",
      "üìù Sample document:\n",
      "   Title: Query-aware Tip Generation for Vertical Search...\n",
      "   ArXiv ID: 2010.09254v1\n",
      "   Content length: 1292 chars\n"
     ]
    }
   ],
   "source": [
    "# Option A: Process Abstracts Only (Recommended for testing)\n",
    "# This is fast and doesn't require PDF downloads\n",
    "\n",
    "print(\"üîÑ Processing abstracts only...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "embedded_docs = process_arxiv_abstracts(max_documents=10)\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(embedded_docs)} documents\")\n",
    "if len(embedded_docs) > 0:\n",
    "    # Handle embedding dimension (can be list or numpy array)\n",
    "    embedding = embedded_docs[0].metadata['embedding']\n",
    "    if hasattr(embedding, 'shape'):\n",
    "        emb_dim = embedding.shape[0] if len(embedding.shape) > 0 else len(embedding)\n",
    "    else:\n",
    "        emb_dim = len(embedding)\n",
    "    \n",
    "    print(f\"üìä Embedding dimension: {emb_dim}\")\n",
    "    print(f\"üìù Sample document:\")\n",
    "    print(f\"   Title: {embedded_docs[0].metadata.get('title', 'N/A')[:60]}...\")\n",
    "    print(f\"   ArXiv ID: {embedded_docs[0].metadata.get('arxiv_id', 'N/A')}\")\n",
    "    print(f\"   Content length: {len(embedded_docs[0].page_content)} chars\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No documents were processed. Please ensure metadata files exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51992dd8",
   "metadata": {},
   "source": [
    "### Option B: Process Full Papers (Uncomment to use)\n",
    "\n",
    "**Note**: This requires PDFs to be downloaded first. Run:\n",
    "```bash\n",
    "python scripts/fetch_arxiv_data.py --max-results 100 --download-pdfs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Process Full Papers (Uncomment to use)\n",
    "# WARNING: This is slow and requires PDFs to be downloaded first!\n",
    "\n",
    "# processor = DocumentProcessor(\n",
    "#     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "#     chunk_strategy=ChunkingStrategy.RECURSIVE\n",
    "# )\n",
    "# \n",
    "# print(\"üîÑ Processing full papers...\")\n",
    "# print(\"=\" * 60)\n",
    "# \n",
    "# embedded_docs = processor.process_documents(\n",
    "#     include_full_text=True,\n",
    "#     max_documents=10,\n",
    "#     save_to_disk=True\n",
    "# )\n",
    "# \n",
    "# print(f\"\\n‚úÖ Processed {len(embedded_docs)} documents\")\n",
    "# if len(embedded_docs) > 0:\n",
    "#     # Handle embedding dimension (can be list or numpy array)\n",
    "#     embedding = embedded_docs[0].metadata['embedding']\n",
    "#     if hasattr(embedding, 'shape'):\n",
    "#         emb_dim = embedding.shape[0] if len(embedding.shape) > 0 else len(embedding)\n",
    "#     else:\n",
    "#         emb_dim = len(embedding)\n",
    "#     \n",
    "#     print(f\"üìä Embedding dimension: {emb_dim}\")\n",
    "#     print(f\"üìù Sample document:\")\n",
    "#     print(f\"   Title: {embedded_docs[0].metadata.get('title', 'N/A')[:60]}...\")\n",
    "#     print(f\"   ArXiv ID: {embedded_docs[0].metadata.get('arxiv_id', 'N/A')}\")\n",
    "#     print(f\"   Content length: {len(embedded_docs[0].page_content)} chars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc09cb",
   "metadata": {},
   "source": [
    "## Step 3: Verify Results\n",
    "\n",
    "Let's check what was saved and verify the processing was successful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Processed chunks directory: /mnt/data/lourvens/learning/research-agent/data/processed/arxiv/chunks\n",
      "üíæ Found 1 chunk files\n",
      "\n",
      "üìÑ Latest chunk file: arxiv_chunks_2025-12-16_10-48-16_abstracts.json\n",
      "   Size: 140.53 KB\n",
      "\n",
      "üìä Chunk file statistics:\n",
      "   Source: arxiv\n",
      "   Total chunks: 10\n",
      "\n",
      "üìù Sample chunk:\n",
      "   Chunk ID: 2010.09254v1_chunk_0\n",
      "   ArXiv ID: 2010.09254v1\n",
      "   Title: Query-aware Tip Generation for Vertical Search...\n",
      "   Has embedding: True\n",
      "   Embedding dim: 384\n"
     ]
    }
   ],
   "source": [
    "# Check processed chunks directory\n",
    "from src.constants import DATA_SOURCE_ARXIV, PROCESSED_CHUNKS_SUBDIR\n",
    "\n",
    "processed_dir = config.PROCESSED_DATA_DIR / DATA_SOURCE_ARXIV / PROCESSED_CHUNKS_SUBDIR\n",
    "chunk_files = list(processed_dir.glob(\"*.json\")) if processed_dir.exists() else []\n",
    "\n",
    "print(f\"üìÅ Processed chunks directory: {processed_dir}\")\n",
    "print(f\"üíæ Found {len(chunk_files)} chunk files\")\n",
    "\n",
    "if len(chunk_files) > 0:\n",
    "    # Get the most recent file\n",
    "    latest_file = max(chunk_files, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"\\nüìÑ Latest chunk file: {latest_file.name}\")\n",
    "    print(f\"   Size: {latest_file.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    # Load and inspect\n",
    "    import json\n",
    "    with latest_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        chunk_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìä Chunk file statistics:\")\n",
    "    print(f\"   Source: {chunk_data.get('source', 'N/A')}\")\n",
    "    print(f\"   Total chunks: {len(chunk_data.get('chunks', []))}\")\n",
    "    \n",
    "    if chunk_data.get('chunks'):\n",
    "        sample_chunk = chunk_data['chunks'][0]\n",
    "        print(f\"\\nüìù Sample chunk:\")\n",
    "        print(f\"   Chunk ID: {sample_chunk.get('chunk_id', 'N/A')}\")\n",
    "        print(f\"   ArXiv ID: {sample_chunk.get('metadata', {}).get('arxiv_id', 'N/A')}\")\n",
    "        print(f\"   Title: {sample_chunk.get('metadata', {}).get('title', 'N/A')[:60]}...\")\n",
    "        print(f\"   Has embedding: {'embedding' in sample_chunk.get('metadata', {})}\")\n",
    "        if 'embedding' in sample_chunk.get('metadata', {}):\n",
    "            emb = sample_chunk['metadata']['embedding']\n",
    "            print(f\"   Embedding dim: {len(emb) if isinstance(emb, list) else 'N/A'}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No chunk files found. Processing may have failed or save_to_disk=False.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfbaff",
   "metadata": {},
   "source": [
    "## Step 4: Inspect Document Metadata\n",
    "\n",
    "Verify that all required metadata fields are preserved (per AGENT.md Rule 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24438739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Required Metadata Fields (AGENT.md Rule 2):\n",
      "============================================================\n",
      "   ‚úÖ source: arxiv\n",
      "   ‚úÖ arxiv_id: 2010.09254v1\n",
      "   ‚úÖ title: Query-aware Tip Generation for Vertical Search\n",
      "   ‚úÖ authors: 10 author(s)\n",
      "   ‚úÖ published: 2020-10-19T06:48:40+00:00\n",
      "   ‚úÖ pdf_url: https://arxiv.org/pdf/2010.09254v1\n",
      "   ‚úÖ embedding: list (dim: 384)\n",
      "\n",
      "‚úÖ All required metadata fields present!\n"
     ]
    }
   ],
   "source": [
    "# Verify metadata completeness\n",
    "if 'embedded_docs' in globals() and len(embedded_docs) > 0:\n",
    "    required_fields = [\n",
    "        'source', 'arxiv_id', 'title', 'authors', \n",
    "        'published', 'pdf_url', 'embedding'\n",
    "    ]\n",
    "    \n",
    "    sample_doc = embedded_docs[0]\n",
    "    metadata = sample_doc.metadata\n",
    "    \n",
    "    print(\"üìã Required Metadata Fields (AGENT.md Rule 2):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    missing_fields = []\n",
    "    for field in required_fields:\n",
    "        if field in metadata:\n",
    "            value = metadata[field]\n",
    "            if field == 'embedding':\n",
    "                # Handle both list and numpy array\n",
    "                if hasattr(value, 'shape'):\n",
    "                    emb_info = f\"{type(value).__name__} (shape: {value.shape})\"\n",
    "                else:\n",
    "                    emb_info = f\"{type(value).__name__} (dim: {len(value)})\"\n",
    "                print(f\"   ‚úÖ {field}: {emb_info}\")\n",
    "            elif field == 'authors':\n",
    "                print(f\"   ‚úÖ {field}: {len(value)} author(s)\")\n",
    "            else:\n",
    "                display_value = str(value)[:50] + \"...\" if len(str(value)) > 50 else str(value)\n",
    "                print(f\"   ‚úÖ {field}: {display_value}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {field}: MISSING\")\n",
    "            missing_fields.append(field)\n",
    "    \n",
    "    if missing_fields:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: {len(missing_fields)} required field(s) missing: {missing_fields}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All required metadata fields present!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No documents to inspect. Run processing cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce8eb61",
   "metadata": {},
   "source": [
    "## Step 5: Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "- ‚úÖ Fetched ArXiv metadata (via script)\n",
    "- ‚úÖ Loaded documents with metadata\n",
    "- ‚úÖ Generated embeddings for document chunks\n",
    "- ‚úÖ Saved processed chunks to disk\n",
    "- ‚úÖ Verified metadata completeness\n",
    "\n",
    "### Next Steps\n",
    "1. **Vector Store**: Create ChromaDB vector store (Phase 1 continuation)\n",
    "2. **RAG Chain**: Build question-answering pipeline\n",
    "3. **Phase 2**: Add multi-source integration\n",
    "\n",
    "### Resources\n",
    "- üìñ [Phase 1 Guide](docs/phase_guides/phase1.md) - Detailed documentation\n",
    "- üìù [AGENT.md](AGENT.md) - Architecture rules\n",
    "- üß™ [Tests](tests/) - Run `pytest tests/ -v` to verify everything works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bacaca",
   "metadata": {},
   "source": [
    "## Step 2: Process Documents\n",
    "\n",
    "Choose one of the following options:\n",
    "\n",
    "### Option A: Process Abstracts Only (Fast, No PDFs Required)\n",
    "- Uses only metadata (title + abstract)\n",
    "- Fast processing (~15 seconds for 100 papers)\n",
    "- No PDF downloads needed\n",
    "\n",
    "### Option B: Process Full Papers (Slower, Requires PDFs)\n",
    "- Includes full PDF text\n",
    "- Slower processing (~5-10 minutes for 100 papers)\n",
    "- Requires PDFs to be downloaded first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34eeccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
